"""
Research-Based Image Quality Validator V4
==========================================
Implements acceptance criteria from deep internet research.

Sources: 40+ scientific papers, industry standards, professional photography guidelines
See: ACCEPTANCE_CRITERIA_RESEARCH.md

Composite Score Formula:
Final_Score = 30% √ó Automated_Metrics + 40% √ó Human_Evaluation + 30% √ó Professional_Standards

Acceptance Threshold: ‚â• 9.0/10 (90%)
"""

import re
from typing import Dict, List, Tuple
from dataclasses import dataclass


@dataclass
class ValidationScore:
    """Score with justification"""
    score: float  # 0-10
    passed: bool
    details: str
    category: str  # "automated", "human", "professional"


class ResearchBasedValidator:
    """
    V4 Validator implementing research-based acceptance criteria.

    Sources:
    - FID < 25 (CVPR 2024, GANs Trained by Humans)
    - CLIP Score > 75 (HuggingFace, OpenAI)
    - Likert 5-point ‚â• 4.0/5.0 (ImageReward MDPI Study)
    - 12 Elements of Merit (Professional Photographers of America)
    - MLOps Quality Gates (atlassian.com)
    """

    def __init__(self):
        self.min_composite_score = 9.0  # Research-based threshold

        # Weight distribution (Source: arxiv.org/html/2503.00625v1)
        # "Humans are ultimate signal receivers of visual content"
        self.weights = {
            "automated": 0.30,   # 30% - Computational metrics
            "human": 0.40,       # 40% - Human perception (highest)
            "professional": 0.30  # 30% - Industry standards
        }

    # ========================================================================
    # NIVEAU 1: AUTOMATED METRICS (Approximated via Prompt Analysis)
    # ========================================================================

    def validate_fid_approximation(self, prompt: str) -> ValidationScore:
        """
        FID (Fr√©chet Inception Distance) < 25

        Source: CVPR 2024 - "GANs Trained by Humans"
        Reference: cvpr.thecvf.com

        Approximation: Check for realism indicators in prompt
        """
        score = 10.0
        issues = []

        # Indicators of realistic images (low FID)
        realism_keywords = [
            r'\braw\b', r'\bcandid\b', r'\bamateur\b', r'\biphone\b',
            r'\bphotorealistic\b', r'\bhyper-realistic\b', r'\bnatural\b',
            r'\bunedited\b', r'\bindistinguishable from real photograph\b'
        ]

        # Anti-patterns (high FID)
        fantasy_keywords = [
            r'\bperfect\b', r'\bflawless\b', r'\bidealized\b',
            r'\bcgi\b', r'\b3d render\b', r'\bartificial\b'
        ]

        realism_count = sum(1 for kw in realism_keywords if re.search(kw, prompt, re.IGNORECASE))
        fantasy_count = sum(1 for kw in fantasy_keywords if re.search(kw, prompt, re.IGNORECASE))

        if realism_count < 2:
            score -= 2.0
            issues.append(f"Insufficient realism keywords ({realism_count}/2 minimum)")

        if fantasy_count > 0:
            score -= 2.0 * fantasy_count
            issues.append(f"Fantasy keywords detected ({fantasy_count})")

        # Check for specific imperfections (correlate with low FID)
        imperfections = [
            r'\bpores\b', r'\bfreckles\b', r'\bblemishes\b',
            r'\bflyaway hair\b', r'\bmessy\b', r'\bimperfect\b'
        ]
        imperfection_count = sum(1 for imp in imperfections if re.search(imp, prompt, re.IGNORECASE))

        if imperfection_count == 0:
            score -= 1.5
            issues.append("No skin/texture imperfections specified")

        details = "‚úÖ FID approximation: LOW (<25 threshold)" if not issues else f"‚ö†Ô∏è Issues: {', '.join(issues)}"
        return ValidationScore(
            score=max(0, score),
            passed=score >= 8.0,
            details=details,
            category="automated"
        )

    def validate_clip_score_approximation(self, prompt: str) -> ValidationScore:
        """
        CLIP Score > 75/100

        Source: HuggingFace Transformers, OpenAI
        Reference: huggingface.co/docs/transformers/model_doc/clip

        Measures text-image alignment. Approximation: Check prompt coherence.
        """
        score = 10.0
        issues = []

        # CLIP measures text-image alignment
        # Good prompts are: specific, detailed, coherent

        # Check specificity (detailed descriptions)
        detail_aspects = {
            "lighting": [r'\blighting\b', r'\bsoft light\b', r'\bnatural light\b', r'\bneon\b', r'\bbedside lamp\b'],
            "camera": [r'\biphone\b', r'\bcanon\b', r'\bnikon\b', r'\bf/\d+\b', r'\bmm lens\b'],
            "context": [r'\bbedroom\b', r'\bcar\b', r'\bmirror\b', r'\bcouch\b', r'\bkitchen\b'],
            "texture": [r'\bskin\b', r'\bhair\b', r'\bfabric\b', r'\btexture\b'],
            "anatomy": [r'\bface\b', r'\bhands\b', r'\beyes\b', r'\blips\b']
        }

        aspects_covered = 0
        for aspect, keywords in detail_aspects.items():
            if any(re.search(kw, prompt, re.IGNORECASE) for kw in keywords):
                aspects_covered += 1

        if aspects_covered < 3:
            score -= 2.0
            issues.append(f"Insufficient detail aspects ({aspects_covered}/5)")

        # Check for contradictions (harm CLIP score)
        words = prompt.lower().split()
        if "professional" in words and "amateur" in words:
            score -= 1.0
            issues.append("Contradictory terms: professional vs amateur")

        if "perfect" in words and "imperfect" in words:
            score -= 1.0
            issues.append("Contradictory terms: perfect vs imperfect")

        # Check length (CLIP prefers detailed but not excessive)
        word_count = len(words)
        if word_count < 20:
            score -= 1.5
            issues.append(f"Too brief ({word_count} words)")
        elif word_count > 100:
            score -= 1.0
            issues.append(f"Too verbose ({word_count} words)")

        details = "‚úÖ CLIP Score approximation: HIGH (>75)" if not issues else f"‚ö†Ô∏è Issues: {', '.join(issues)}"
        return ValidationScore(
            score=max(0, score),
            passed=score >= 8.0,
            details=details,
            category="automated"
        )

    def validate_brisque_approximation(self, prompt: str) -> ValidationScore:
        """
        BRISQUE (No-Reference Quality) < 40

        Source: MATLAB Image Processing Toolbox
        Reference: mathworks.com/help/images/ref/brisque.html

        Measures image quality without reference. Lower = better.
        Approximation: Check for quality-harming factors.
        """
        score = 10.0
        issues = []

        # Factors that increase BRISQUE (worsen quality)
        quality_degraders = [
            r'\bblurry\b', r'\bnoisy\b', r'\bgrainy\b',
            r'\bcompressed\b', r'\blow resolution\b', r'\bpixelated\b',
            r'\bartifacts\b', r'\bdistorted\b'
        ]

        # Factors that decrease BRISQUE (improve quality)
        quality_enhancers = [
            r'\bsharp\b', r'\bcrisp\b', r'\bclear\b', r'\bhd\b',
            r'\bhigh resolution\b', r'\bdetailed\b', r'\b4k\b', r'\b8k\b'
        ]

        degrader_count = sum(1 for dg in quality_degraders if re.search(dg, prompt, re.IGNORECASE))
        enhancer_count = sum(1 for eh in quality_enhancers if re.search(eh, prompt, re.IGNORECASE))

        if degrader_count > 0:
            score -= 3.0 * degrader_count
            issues.append(f"Quality degraders present ({degrader_count})")

        if enhancer_count == 0:
            score -= 1.0
            issues.append("No quality enhancers specified")

        # Check for "amateur" style (slightly increases BRISQUE but acceptable)
        if re.search(r'\bamateur\b', prompt, re.IGNORECASE):
            # Amateur is OK if balanced with "sharp" or "clear"
            if not any(re.search(kw, prompt, re.IGNORECASE) for kw in [r'\bsharp\b', r'\bclear\b']):
                score -= 0.5
                issues.append("Amateur style without clarity specification")

        details = "‚úÖ BRISQUE approximation: LOW (<40)" if not issues else f"‚ö†Ô∏è Issues: {', '.join(issues)}"
        return ValidationScore(
            score=max(0, score),
            passed=score >= 8.0,
            details=details,
            category="automated"
        )

    # ========================================================================
    # NIVEAU 2: HUMAN EVALUATION (Simulated via Comprehensive Checklist)
    # ========================================================================

    def validate_likert_5point(self, prompt: str) -> ValidationScore:
        """
        Likert 5-point Scale ‚â• 4.0/5.0 (80%)

        Source: ImageReward MDPI Study 2024
        Reference: mdpi.com/2076-3417/14/3/1219

        Question: "How realistic does this image look?"
        1=Very Unrealistic, 2=Unrealistic, 3=Neutral, 4=Realistic, 5=Very Realistic

        Target: ‚â• 4.0 (Realistic to Very Realistic)
        """
        score = 10.0
        issues = []

        # Factors humans rate as "realistic"
        human_realism_checklist = {
            "Natural imperfections": [r'\bpores\b', r'\bfreckles\b', r'\bblemishes\b', r'\bwrinkles\b'],
            "Organic hair": [r'\bmessy\b', r'\bflyaway\b', r'\bindividual hair strands\b', r'\bnatural hair\b'],
            "Casual context": [r'\bamateur\b', r'\bcasual\b', r'\bcandid\b', r'\bsnapshot\b', r'\bselfie\b'],
            "Realistic lighting": [r'\bnatural light\b', r'\bwindow light\b', r'\bbedside lamp\b', r'\bsoft light\b'],
            "Authenticity markers": [r'\braw\b', r'\bunedited\b', r'\bunfiltered\b', r'\bauthentic\b']
        }

        passed_checks = 0
        for check_name, keywords in human_realism_checklist.items():
            if any(re.search(kw, prompt, re.IGNORECASE) for kw in keywords):
                passed_checks += 1
            else:
                issues.append(f"Missing: {check_name}")

        # Score calculation (need 4/5 checks = 80%)
        if passed_checks < 3:
            score -= 3.0
        elif passed_checks < 4:
            score -= 1.5

        # Penalize "fantasy" indicators (humans rate as unrealistic)
        fantasy_indicators = [
            r'\bperfect\b', r'\bflawless\b', r'\bphotoshopped\b',
            r'\bmodel-like\b', r'\binstagram perfect\b'
        ]

        fantasy_count = sum(1 for fi in fantasy_indicators if re.search(fi, prompt, re.IGNORECASE))
        if fantasy_count > 0:
            score -= 2.0 * fantasy_count
            issues.append(f"Fantasy indicators detected ({fantasy_count})")

        likert_estimate = 5.0 * (score / 10.0)  # Convert to Likert scale
        details = f"‚úÖ Likert 5-point: {likert_estimate:.1f}/5.0 (‚â•4.0 target)" if not issues else f"‚ö†Ô∏è Estimate: {likert_estimate:.1f}/5.0 - Issues: {', '.join(issues[:2])}"

        return ValidationScore(
            score=max(0, score),
            passed=score >= 8.0,  # 8/10 = 4.0/5.0 Likert
            details=details,
            category="human"
        )

    def validate_continuous_scale(self, prompt: str) -> ValidationScore:
        """
        Continuous Scale 0-100 ‚â• 70/100

        Source: RAISE Database Human Study
        Reference: arxiv.org/abs/1505.03632

        "Overall photorealism quality" - Humans rate on continuous slider
        Target: ‚â• 70/100 (Good to Excellent)
        """
        score = 10.0
        issues = []

        # Comprehensive quality factors humans consider
        quality_dimensions = {
            "Anatomical correctness": {
                "keywords": [r'\b5 fingers\b', r'\bsymmetric face\b', r'\bproportional\b', r'\banatomy\b'],
                "weight": 2.0
            },
            "Lighting quality": {
                "keywords": [r'\bsoft\b', r'\bnatural\b', r'\bdirectional\b', r'\bspecific light\b'],
                "weight": 1.5
            },
            "Texture detail": {
                "keywords": [r'\bpores\b', r'\btexture\b', r'\bdetailed skin\b', r'\bfabric weave\b'],
                "weight": 1.5
            },
            "Context coherence": {
                "keywords": [r'\bbedroom\b', r'\bkitchen\b', r'\bcar interior\b', r'\bspecific location\b'],
                "weight": 1.0
            },
            "Photographic style": {
                "keywords": [r'\bcamera\b', r'\blens\b', r'\bf/\b', r'\bmm\b', r'\bshot on\b'],
                "weight": 1.0
            }
        }

        total_penalty = 0.0
        for dimension, config in quality_dimensions.items():
            has_keywords = any(re.search(kw, prompt, re.IGNORECASE) for kw in config["keywords"])
            if not has_keywords:
                total_penalty += config["weight"]
                issues.append(f"Missing: {dimension}")

        score -= total_penalty

        continuous_estimate = 100 * (score / 10.0)  # Convert to 0-100 scale
        details = f"‚úÖ Continuous Scale: {continuous_estimate:.0f}/100 (‚â•70 target)" if score >= 7.0 else f"‚ö†Ô∏è Estimate: {continuous_estimate:.0f}/100 - Issues: {', '.join(issues[:3])}"

        return ValidationScore(
            score=max(0, score),
            passed=score >= 7.0,  # 7/10 = 70/100
            details=details,
            category="human"
        )

    # ========================================================================
    # NIVEAU 3: PROFESSIONAL STANDARDS
    # ========================================================================

    def validate_12_elements_merit(self, prompt: str) -> ValidationScore:
        """
        12 Elements of Merit (Professional Photography)

        Source: Professional Photographers of America (PPA)
        Reference: ppam.org

        Professional image quality standard. Checking key elements:
        1. Impact, 2. Technical Excellence, 3. Composition, 4. Lighting,
        5. Subject Matter, 6. Presentation, 7. Color Balance, 8. Center of Interest,
        9. Style, 10. Technique, 11. Storytelling, 12. Creativity
        """
        score = 10.0
        issues = []

        # Key elements we can verify from prompt
        merit_elements = {
            "Technical Excellence (sharpness, focus)": [r'\bsharp\b', r'\bcrisp\b', r'\bfocused\b', r'\bclear\b'],
            "Lighting (quality, direction)": [r'\blighting\b', r'\blight\b', r'\bsoft light\b', r'\bwindow\b'],
            "Composition (framing, perspective)": [r'\bportrait\b', r'\bclose-up\b', r'\bfull body\b', r'\bangle\b'],
            "Subject Matter (clear subject)": [r'\bwoman\b', r'\bgirl\b', r'\bperson\b', r'\bmodel\b'],
            "Color Balance": [r'\bwarm tones\b', r'\bcool tones\b', r'\bnatural color\b', r'\bcolor\b'],
            "Style (consistent aesthetic)": [r'\bamateur\b', r'\bprofessional\b', r'\bcasual\b', r'\braw\b']
        }

        elements_present = 0
        for element, keywords in merit_elements.items():
            if any(re.search(kw, prompt, re.IGNORECASE) for kw in keywords):
                elements_present += 1
            else:
                issues.append(element.split("(")[0].strip())

        # Need at least 4/6 elements (67%)
        if elements_present < 3:
            score -= 3.0
        elif elements_present < 4:
            score -= 1.5

        # Bonus for specificity (professional touch)
        specific_details = [
            r'\bf/\d+\.\d+\b',  # aperture
            r'\b\d+mm\b',       # focal length
            r'\bISO\s*\d+\b',   # ISO
            r'\b(Canon|Nikon|Sony|Fujifilm)\b'  # camera brand
        ]

        specificity = sum(1 for detail in specific_details if re.search(detail, prompt, re.IGNORECASE))
        if specificity >= 2:
            score = min(10.0, score + 0.5)  # Bonus for professional specs

        details = f"‚úÖ 12 Elements: {elements_present}/6 key elements present" if score >= 7.0 else f"‚ö†Ô∏è {elements_present}/6 elements - Missing: {', '.join(issues[:2])}"

        return ValidationScore(
            score=max(0, score),
            passed=score >= 7.0,
            details=details,
            category="professional"
        )

    def validate_technical_quality_gates(self, prompt: str) -> ValidationScore:
        """
        MLOps Quality Gates (Production Readiness)

        Source: Atlassian MLOps Guide
        Reference: atlassian.com/devops/devops-tools/mlops

        Production acceptance criteria for ML systems.
        Must pass ALL gates for deployment.
        """
        score = 10.0
        gates_passed = []
        gates_failed = []

        # Gate 1: Functional Requirements
        if re.search(r'\bphotorealistic\b|\bhyper-realistic\b|\breal photograph\b', prompt, re.IGNORECASE):
            gates_passed.append("Functional (photorealism)")
        else:
            score -= 2.0
            gates_failed.append("Functional")

        # Gate 2: Performance (quality expectations)
        quality_keywords = [r'\bhigh quality\b', r'\bdetailed\b', r'\bcrisp\b', r'\bsharp\b']
        if any(re.search(kw, prompt, re.IGNORECASE) for kw in quality_keywords):
            gates_passed.append("Performance")
        else:
            score -= 2.0
            gates_failed.append("Performance")

        # Gate 3: Safety (NSFW appropriately tagged)
        # For production: NSFW content MUST be explicitly specified
        nsfw_indicators = [r'\bnude\b', r'\btopless\b', r'\bNSFW\b', r'\bexplicit\b']
        has_nsfw = any(re.search(ind, prompt, re.IGNORECASE) for ind in nsfw_indicators)

        # If NSFW content, must have explicit markers
        if has_nsfw:
            gates_passed.append("Safety (NSFW tagged)")
        else:
            # SFW content is OK too
            gates_passed.append("Safety (SFW)")

        # Gate 4: Consistency (style specification)
        style_keywords = [r'\bstyle\b', r'\bamateur\b', r'\bprofessional\b', r'\bcasual\b', r'\braw\b']
        if any(re.search(kw, prompt, re.IGNORECASE) for kw in style_keywords):
            gates_passed.append("Consistency")
        else:
            score -= 1.5
            gates_failed.append("Consistency")

        # Gate 5: Reproducibility (specific parameters)
        has_params = bool(re.search(r'\bf/\d+|\biPhone|\bCanon|\bmm\b|\bISO\b', prompt, re.IGNORECASE))
        if has_params:
            gates_passed.append("Reproducibility")
        else:
            score -= 1.5
            gates_failed.append("Reproducibility")

        gates_status = f"‚úÖ {len(gates_passed)}/5 gates PASSED"
        if gates_failed:
            gates_status += f" | ‚ùå Failed: {', '.join(gates_failed)}"

        return ValidationScore(
            score=max(0, score),
            passed=len(gates_failed) == 0,  # ALL gates must pass
            details=gates_status,
            category="professional"
        )

    # ========================================================================
    # COMPOSITE VALIDATION
    # ========================================================================

    def validate_comprehensive(self, prompt: str) -> Dict:
        """
        Run ALL validation criteria and compute composite score.

        Returns:
            {
                "composite_score": float (0-10),
                "passed": bool,
                "category_scores": {...},
                "detailed_results": [...],
                "recommendation": str
            }
        """

        # Run all validators
        validators = {
            # Automated
            "FID Approximation": self.validate_fid_approximation(prompt),
            "CLIP Score Approximation": self.validate_clip_score_approximation(prompt),
            "BRISQUE Approximation": self.validate_brisque_approximation(prompt),

            # Human
            "Likert 5-point": self.validate_likert_5point(prompt),
            "Continuous 0-100": self.validate_continuous_scale(prompt),

            # Professional
            "12 Elements of Merit": self.validate_12_elements_merit(prompt),
            "Technical Quality Gates": self.validate_technical_quality_gates(prompt),
        }

        # Calculate category averages
        category_scores = {}
        for category in ["automated", "human", "professional"]:
            category_results = [v for v in validators.values() if v.category == category]
            if category_results:
                category_scores[category] = sum(r.score for r in category_results) / len(category_results)
            else:
                category_scores[category] = 0.0

        # Compute weighted composite score
        composite_score = (
            self.weights["automated"] * category_scores["automated"] +
            self.weights["human"] * category_scores["human"] +
            self.weights["professional"] * category_scores["professional"]
        )

        # Determine pass/fail
        passed = composite_score >= self.min_composite_score

        # Generate recommendation
        if composite_score >= 9.5:
            recommendation = "üéâ EXCELLENT - Production Ready"
        elif composite_score >= 9.0:
            recommendation = "‚úÖ PASSED - Meets acceptance criteria"
        elif composite_score >= 8.0:
            recommendation = "‚ö†Ô∏è GOOD - Minor improvements needed"
        else:
            recommendation = "‚ùå FAILED - Significant improvements required"

        # Identify weakest areas
        failed_validators = [name for name, result in validators.items() if not result.passed]

        return {
            "composite_score": round(composite_score, 2),
            "passed": passed,
            "category_scores": {k: round(v, 2) for k, v in category_scores.items()},
            "detailed_results": {name: {
                "score": round(result.score, 2),
                "passed": result.passed,
                "details": result.details,
                "category": result.category
            } for name, result in validators.items()},
            "failed_validators": failed_validators,
            "recommendation": recommendation,
            "threshold": self.min_composite_score
        }


def print_validation_report(prompt: str, results: Dict):
    """Pretty print validation results"""

    print("\n" + "="*80)
    print("üî¨ RESEARCH-BASED VALIDATION REPORT")
    print("="*80)
    print(f"\nüìù Prompt (first 100 chars): {prompt[:100]}...")
    print(f"\n{'='*80}")
    print(f"üéØ COMPOSITE SCORE: {results['composite_score']:.2f}/10")
    print(f"‚úÖ STATUS: {'PASSED ‚úì' if results['passed'] else 'FAILED ‚úó'}")
    print(f"üìä Threshold: ‚â• {results['threshold']:.1f}/10")
    print(f"{'='*80}")

    # Category breakdown
    print("\nüìà CATEGORY SCORES:")
    print(f"  ‚Ä¢ Automated Metrics:      {results['category_scores']['automated']:.2f}/10 (Weight: 30%)")
    print(f"  ‚Ä¢ Human Evaluation:       {results['category_scores']['human']:.2f}/10 (Weight: 40%)")
    print(f"  ‚Ä¢ Professional Standards: {results['category_scores']['professional']:.2f}/10 (Weight: 30%)")

    # Detailed results
    print(f"\n{'='*80}")
    print("üìã DETAILED VALIDATION RESULTS:")
    print(f"{'='*80}")

    for name, result in results['detailed_results'].items():
        status = "‚úÖ" if result['passed'] else "‚ùå"
        print(f"\n{status} {name}")
        print(f"   Score: {result['score']:.2f}/10")
        print(f"   {result['details']}")

    # Failed validators
    if results['failed_validators']:
        print(f"\n{'='*80}")
        print("‚ö†Ô∏è  FAILED VALIDATORS:")
        for validator in results['failed_validators']:
            print(f"  ‚Ä¢ {validator}")

    # Recommendation
    print(f"\n{'='*80}")
    print(f"üí° RECOMMENDATION: {results['recommendation']}")
    print(f"{'='*80}\n")


# Testing
if __name__ == "__main__":
    import sys
    import io

    # Fix Windows encoding
    if sys.platform == 'win32':
        sys.stdout.reconfigure(encoding='utf-8')

    print("\nüöÄ Testing Research-Based Validator V4\n")

    validator = ResearchBasedValidator()

    # Test cases
    test_prompts = [
        # Good prompt (should score ~9+)
        """raw candid amateur photo of a beautiful East Asian woman in her mid 20s with oval face shape,
        shot on iPhone 13, natural window lighting from left side, casual bedroom setting with messy bed in background,
        woman wearing black lace lingerie, visible skin pores and natural freckles across nose and cheeks,
        individual hair strands visible with some flyaway hairs, soft natural smile, 5 fingers on each hand visible,
        symmetric facial features, photorealistic, hyper-realistic, looks indistinguishable from real amateur photograph,
        unedited, authentic, intimate mood""",

        # Bad prompt (should score low)
        """perfect beautiful woman, flawless skin, professional studio photo, ideal proportions,
        fantasy style, CGI quality, Instagram perfect""",
    ]

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'#'*80}")
        print(f"TEST {i}/{len(test_prompts)}")
        print(f"{'#'*80}")

        results = validator.validate_comprehensive(prompt)
        print_validation_report(prompt, results)

        print("\n" + "="*80 + "\n")
